{"metadata":{"dataExplorerConfig":[],"bento_stylesheets":{"bento\/extensions\/flow\/main.css":true,"bento\/extensions\/kernel_selector\/main.css":true,"bento\/extensions\/kernel_ui\/main.css":true,"bento\/extensions\/new_kernel\/main.css":true,"bento\/extensions\/system_usage\/main.css":true,"bento\/extensions\/theme\/main.css":true},"kernelspec":{"name":"bento_kernel_pytorch","display_name":"pytorch","language":"python","metadata":{"kernel_name":"bento_kernel_pytorch","nightly_builds":true,"fbpkg_supported":true,"cinder_runtime":true,"is_prebuilt":true,"default_tagged_version":"2200","kernel_version":"2200"},"isCinder":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text\/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"last_server_session_id":"f147802e-25f5-4135-8d87-36dfe4f34445","last_kernel_id":"38c3da83-e443-489a-a812-c7905ecd4943","last_base_url":"https:\/\/bento.edge.x2p.facebook.net\/","last_msg_id":"1db3947c-012918d059de8646fe6205a5_9827","outputWidgetContext":[]},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"code","metadata":{"collapsed":false,"originalKey":"2aff5df8-f8b0-4e4b-901a-1c95f4d93b79","outputsInitialized":true,"executionStartTime":1706946645903,"executionStopTime":1706946648613,"serverExecutionDuration":2555.8648779988,"requestMsgId":"722b6013-7a1e-4f45-ae34-2a6379b77d04","output":{"id":"343664195313822"}},"source":["import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\u0025matplotlib inline"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["I0202 235046.167 _utils_internal.py:234] NCCL_DEBUG env var is set to None\n"]},{"output_type":"stream","name":"stderr","text":["I0202 235046.168 _utils_internal.py:252] NCCL_DEBUG is forced to WARN from None\n"]},{"output_type":"stream","name":"stderr","text":["I0202 235048.446 font_manager.py:1349] generated new fontManager\n"]}]},{"cell_type":"code","metadata":{"originalKey":"18c073db-07d7-43d9-abda-9b5d5b6531a4","showInput":true,"customInput":null,"executionStartTime":1706946652214,"executionStopTime":1706946652388,"serverExecutionDuration":6.6080589895137,"collapsed":false,"requestMsgId":"a93cec58-9eb8-4fb4-bf96-e01364851c3f","outputsInitialized":true,"output":{"id":"2262924433904192"}},"source":["# load data\n","\n","words = open(\"\/home\/yucongguo\/names.txt\", \"r\").read().splitlines()\n","print(words[:8])\n","print(f\"length of words = {len(words)}\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\nlength of words = 32033\n"]}]},{"cell_type":"code","metadata":{"originalKey":"07d76ba3-4591-4e97-87a6-d00b42992b6a","showInput":true,"customInput":null,"executionStartTime":1706947032254,"executionStopTime":1706947032443,"serverExecutionDuration":8.1697159912437,"collapsed":false,"requestMsgId":"17fbd7c9-7c76-4e90-8a73-08e875882624","outputsInitialized":true,"output":{"id":"923120929521122"}},"source":["# build the vocabulary of characters and mapping to\/from integers\n","\n","char = sorted(list(set(''.join(words))))\n","str_to_int = {string: index+1 for index, string in enumerate(char)}\n","str_to_int['.'] = 0\n","int_to_str = {index+1: string for index, string in enumerate(char)}\n","int_to_str[0] = '.'\n","vocab_size = len(int_to_str)\n","print(f\"str_to_int : {str_to_int}\\n\")\n","print(f\"int_to_str : {int_to_str}\\n\")\n","print(f\"vocab_size : {vocab_size}\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["str_to_int : {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n\nint_to_str : {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n\nvocab_size : 27\n"]}]},{"cell_type":"code","metadata":{"originalKey":"a0e1f4dc-9c23-430a-9053-8af78c0037ef","showInput":true,"customInput":null,"executionStartTime":1706947205527,"executionStopTime":1706947206126,"serverExecutionDuration":371.28219101578,"collapsed":false,"requestMsgId":"b3a77dd5-f148-467d-a857-73dac252e3ff","outputsInitialized":true,"output":{"id":"922359839149277"}},"source":["# build the dataset\n","\n","block_size = 3 # context length: how many characters do we take to predict the next one?\n","\n","def build_dataset(words):\n","    #input: list of wards\n","    #output: X,Y\n","\n","    x = []\n","    y = []\n","    for w in words:\n","        context = [0] * block_size #assume all word start with \"...\"\n","        for character in w+'.':\n","            next_char_index = str_to_int[character]\n","            x.append(context)\n","            y.append(next_char_index)\n","            context = context[1:] + [next_char_index]\n","\n","    x = torch.tensor(x)\n","    y = torch.tensor(y)\n","\n","    return x, y\n","\n","import random\n","random.seed(42)\n","random.shuffle(words)\n","\n","n1 = int(0.8 * len(words))\n","n2 = int(0.9 * len(words))\n","\n","x_train,y_train = build_dataset(words[:n1])\n","x_val,y_val = build_dataset(words[n1:n2])\n","x_test,y_test = build_dataset(words[n2:])\n","\n","print(f\"x_train shape : {x_train.shape}, y_train shape: {y_train.shape}\")\n","print(f\"x_val shape : {x_val.shape}, y_val shape: {y_val.shape}\")\n","print(f\"x_test shape : {x_test.shape}, y_test shape: {y_test.shape}\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape : torch.Size([182437, 3]), y_train shape: torch.Size([182437])\nx_val shape : torch.Size([22781, 3]), y_val shape: torch.Size([22781])\nx_test shape : torch.Size([22928, 3]), y_test shape: torch.Size([22928])\n"]}]},{"cell_type":"code","metadata":{"originalKey":"c5a841f1-b7f0-40ac-9f09-af27c8a8df79","showInput":true,"customInput":null,"executionStartTime":1706999834490,"executionStopTime":1706999834680,"serverExecutionDuration":1.6903910436668,"collapsed":false,"requestMsgId":"38923447-cfb4-46d0-8d2f-11d7c3b31abf","outputsInitialized":true},"source":["# utility function we will use later when comparing manual gradients to PyTorch gradients\n","def cmp(s, dt, t):\n","  ex = torch.all(dt == t.grad).item() #return true of false\n","  app = torch.allclose(dt, t.grad) #return true of false\n","  maxdiff = (dt - t.grad).abs().max().item()\n","  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"],"execution_count":79,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"e56ff2e8-2237-4fd2-8d6c-b838eacc86b6","showInput":false,"customInput":null},"source":["## forward pass"]},{"cell_type":"code","metadata":{"originalKey":"d30ee026-c1d3-4be7-b608-89ab49483c2d","showInput":true,"customInput":null,"executionStartTime":1706999983951,"executionStopTime":1706999984125,"serverExecutionDuration":4.9465440097265,"collapsed":false,"requestMsgId":"a9a20ca6-8d7b-48b4-83c9-1aad8c8811e5","outputsInitialized":true,"output":{"id":"766499451765988"}},"source":["# define parameters\n","\n","n_embd = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n","#vocal_size = 27\n","#block_size = 3\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C  = torch.randn((vocab_size, n_embd),            generator=g)\n","# Layer 1\n","W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5\/3)\/((n_embd * block_size)**0.5)\n","b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n","b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n","# BatchNorm parameters\n","bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden))*0.1\n","\n","# Note: I am initializating many of these parameters in non-standard ways\n","# because sometimes initializating with e.g. all zeros could mask an incorrect\n","# implementation of the backward pass.\n","\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","  p.requires_grad = True"],"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["4137\n"]}]},{"cell_type":"code","metadata":{"originalKey":"134f57ee-9535-4716-a356-55556943b922","showInput":true,"customInput":null,"executionStartTime":1707001036738,"executionStopTime":1707001036950,"serverExecutionDuration":5.3148940205574,"collapsed":false,"requestMsgId":"c367d316-bda2-414f-9557-412c09b7aa0f","outputsInitialized":true,"output":{"id":"1394890071144876"}},"source":["# forward pass\n","\n","batch_size = 32\n","ix = torch.randint(0, x_train.shape[0],(batch_size,), generator = g)\n","x_train_batch, y_train_batch = x_train[ix], y_train[ix] # batch X,Y\n","\n","emb = C[x_train_batch] # embed the characters into vectors\n","embcat = emb.view(emb.shape[0],-1) # reshape into a tensor of size (32,30)\n","# Linear layer 1\n","hidden_pre_batchnorm = embcat \u0040 W1 + b1 # hidden layer pre-BatchNorm  (64,1) = (32,30) *(30,64) + (64,1)\n","# BatchNorm layer\n","bnmeani = 1\/batch_size * hidden_pre_batchnorm.sum(0, keepdim=True) #minibatch mean\n","bndiff = hidden_pre_batchnorm - bnmeani\n","bndiff2 = bndiff**2\n","bnvar = 1\/(batch_size-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n","bnvar_inv = (bnvar + 1e-5)**-0.5\n","bnraw = bndiff * bnvar_inv\n","hidden_pre_act = bngain * bnraw + bnbias # scale and shift normalized layer\n","\n","# Non-linearity\n","h = torch.tanh(hidden_pre_act) # hidden layer\n","# Linear layer 2\n","logits = h \u0040 W2 + b2 # output layer\n","#cross entropy loss (same as F.cross_entropy(logits, Yb))\n","logit_maxes = logits.max(1, keepdim=True).values\n","norm_logits = logits - logit_maxes # subtract max for numerical stability\n","#probs  = norm_logits.exp() \/ norm_logits.exp().sum(1, keepdims=True) #softmax\n","counts = norm_logits.exp()\n","counts_sum = counts.sum(1, keepdims=True)\n","counts_sum_inv = counts_sum**-1 # if I use (1.0 \/ counts_sum) instead then I can't get backprop to be bit exact...\n","probs = counts * counts_sum_inv\n","logprobs = probs.log() #(32,27)\n","loss = -logprobs[range(batch_size), y_train_batch].mean() # (x1+x2+x3+x4+..+x32) \/ 32\n","\n","\n","print(f\"loss = {loss}\")"],"execution_count":103,"outputs":[{"output_type":"stream","name":"stdout","text":["loss = 3.4124536514282227\n"]}]},{"cell_type":"markdown","metadata":{"originalKey":"b8ace309-ef97-441c-9b32-b09c48270d0d","showInput":false,"customInput":null},"source":["## backprop "]},{"cell_type":"code","metadata":{"originalKey":"f324c148-06bf-4ad1-b913-89b5bd364106","showInput":true,"customInput":null,"executionStartTime":1707001069627,"executionStopTime":1707001069768,"serverExecutionDuration":11.684896016959,"collapsed":false,"requestMsgId":"947ea5f6-b12c-45eb-81c3-9fc4162bd0a7","outputsInitialized":true,"output":{"id":"357555993841522"}},"source":["# PyTorch backward pass\n","for p in parameters:\n","  p.grad = None\n","for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n","          norm_logits, logit_maxes, logits, h, hidden_pre_act, bnraw,\n","         bnvar_inv, bnvar, bndiff2, bndiff, hidden_pre_batchnorm, bnmeani,\n","         embcat, emb]:\n","  t.retain_grad()\n","loss.backward()\n","loss\n",""],"execution_count":105,"outputs":[{"output_type":"execute_result","data":{"text\/plain":"tensor(3.4125, grad_fn=\u003CNegBackward0>)"},"metadata":{},"execution_count":105}]},{"cell_type":"code","metadata":{"originalKey":"9f351723-c897-40ba-93d2-3e25fa9cf04a","showInput":true,"customInput":null,"executionStartTime":1707001074513,"executionStopTime":1707001074662,"serverExecutionDuration":6.747800973244,"collapsed":false,"requestMsgId":"0df9ca3f-3308-4c6f-b741-ef6145896adb","outputsInitialized":true,"output":{"id":"402303182327954"}},"source":["#backprop of softmax function\n","\n","\n","loss = -logprobs[range(batch_size), y_train_batch].mean() # (x1+x2+x3+x4+..+x32) \/ 32\n","\n","d_loss= 1\n","\n","'''\n","loss = -logprobs[range(batch_size), y_train_batch].mean() # (x1+x2+x3+x4+..+x32) \/ 32\n","'''\n","d_logprobs = torch.zeros_like(logprobs)\n","d_logprobs[range(batch_size), y_train_batch] = -1.0 \/ batch_size #since (x1+x2+x3+x4+..+x32) \/ 32, then dx1 = 1\/32\n","d_logprobs = d_loss *d_logprobs #(32,27)\n","\n","'''\n","logprobs = probs.log() \n","logprobs: (32,27)\n","probs: (32,27)\n","'''\n","d_probs = d_logprobs *  (1\/probs)   # (32,27) = (32,27) * 1\/(32,27) | y=logx,  dy\/dx = 1\/x\n","\n","'''\n","probs = counts * counts_sum_inv  \n","probs: (32,27)\n","counts: (32,27)\n","counts_sum_inv: (32,1)\n","'''\n","d_counts = d_probs * counts_sum_inv  # (32,27) * (32,1) = (32,27 )\n","d_counts_sum_inv = (d_probs * counts).sum(1, keepdim = True) #(32,1)\n","\n","'''\n","counts_sum_inv = counts_sum**-1 \n","counts_sum_inv: (32,1)\n","counts_sum: (32,1)\n","'''\n","d_counts_sum = d_counts_sum_inv * (-counts_sum**-2)  #(32,1)\n","\n","'''\n","counts_sum = counts.sum(1, keepdims=True)\n","counts_sum: (32,1)\n","counts: (32,27)\n","'''\n","d_counts += d_counts_sum * torch.ones_like(counts)  #(32,1) * (32,27) = (32,27)\n","# a11 a12 a13 ----> b1 = a11+a12+a13.     therefore, the deravete is 1\n","# a21 a22 a23 ----> b2 = a21+a22+a23\n","# a31 a32 a33 ----> b3 = a31+a32+a33\n","\n","'''\n","counts = norm_logits.exp()\n","counts: (32,27)\n","norm_logits : (32,27)\n","'''\n","d_norm_logits = d_counts * counts\n","\n","'''\n","norm_logits = logits - logit_maxes # subtract max for numerical stability\n","(32,27)\n","'''\n","d_logits = d_norm_logits.clone()\n","\n","cmp('logprobs', d_logprobs, logprobs)\n","cmp('probs', d_probs, probs)\n","cmp('counts_sum_inv', d_counts_sum_inv, counts_sum_inv)\n","cmp('counts_sum', d_counts_sum, counts_sum)\n","cmp('counts', d_counts, counts)\n","cmp('norm_logits', d_norm_logits, norm_logits)\n","cmp('logits', d_logits, logits)"],"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\nprobs           | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\ncounts          | exact: True  | approximate: True  | maxdiff: 0.0\nnorm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\nlogits          | exact: False | approximate: True  | maxdiff: 7.2177499532699585e-09\n"]}]},{"cell_type":"code","metadata":{"originalKey":"c10b2433-62b2-4c6b-8af1-799d0a8d0f79","showInput":true,"customInput":null,"executionStartTime":1707002020096,"executionStopTime":1707002020237,"serverExecutionDuration":14.835707028396,"collapsed":false,"requestMsgId":"957ddb93-fe96-4b9b-9435-6c5284ee4fb8","outputsInitialized":true,"output":{"id":"1093196251821678"}},"source":["# backprop of model\n","\n","\n","'''\n","logits = h \u0040 W2 + b2 \n","logits: (32,27)\n","h : (32,64)\n","w2:(62,27)\n","b2: (32,1)\n","'''\n","d_h = d_logits \u0040 W2.T #(32,27) \u0040 (27,64) = (32,64)\n","d_W2 = h.T \u0040 d_logits #(64,32) \u0040 (32,27) = (64,27)\n","d_b2 = d_logits.sum(0)\n","\n","'''\n","h = torch.tanh(hidden_pre_act) # hidden layer\n","h : (32,64)\n","hidden_pre_act : (32,64)\n","'''\n","d_hidden_pre_act = (1.0 - h**2) * d_h\n","\n","'''\n","hidden_pre_act = bngain * bnraw + bnbias # scale and shift normalized layer\n","hidden_pre_act : (32,64)\n","bngain : (1,64)\n","bnraw : (32,64)\n","bnbias : (1,64)\n","'''\n","d_bngain = (bnraw * d_hidden_pre_act).sum(0, keepdim=True) \n","d_bnraw = bngain * d_hidden_pre_act\n","d_bnbias = d_hidden_pre_act.sum(0, keepdim=True)\n","\n","'''\n","bnraw = bndiff * bnvar_inv\n","bnraw : (32,64)\n","bndiff : (32,64)\n","bnvar_inv : (1,64)\n","'''\n","d_bndiff = bnvar_inv * d_bnraw\n","d_bnvar_inv = (bndiff * d_bnraw).sum(0, keepdim=True)\n","\n","'''\n","bnvar_inv = (bnvar + 1e-5)**-0.5\n","bnvar_inv : (1,64)\n","bnvar : (1,64)\n","'''\n","d_bnvar = (-0.5*(bnvar + 1e-5)**-1.5) * d_bnvar_inv\n","\n","'''\n","bnvar = 1\/(batch_size-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n","bnvar : (1,64)\n","bndiff2: (32,64)\n","'''\n","d_bndiff2 = (1.0\/(batch_size-1))*torch.ones_like(bndiff2) * d_bnvar\n","\n","'''\n","bndiff2 = bndiff**2\n","'''\n","d_bndiff += (2*bndiff) * d_bndiff2\n","\n","'''\n","bndiff = hidden_pre_batchnorm - bnmeani\n","bndiff: (32,64)\n","hidden_pre_batchnorm: (32,64)\n","bnmeani: (1,64)\n","'''\n","d_hidden_pre_batchnorm = d_bndiff.clone()\n","d_bnmeani = (-d_bndiff).sum(0)\n","\n","'''\n","bnmeani = 1\/batch_size * hidden_pre_batchnorm.sum(0, keepdim=True) #minibatch mean\n","bnmeani: (1,64)\n","hidden_pre_batchnorm: (32,64)\n","'''\n","d_hidden_pre_batchnorm += 1.0\/batch_size * (torch.ones_like(hidden_pre_batchnorm) * d_bnmeani)\n","\n","'''\n","hidden_pre_batchnorm = embcat \u0040 W1 + b1 # hidden layer pre-BatchNorm  (64,1) = (32,30) *(30,64) + (64,1)\n","'''\n","d_embcat = d_hidden_pre_batchnorm \u0040 W1.T\n","d_W1 = embcat.T \u0040 d_hidden_pre_batchnorm\n","d_b1 = d_hidden_pre_batchnorm.sum(0)\n","\n","'''\n","embcat = emb.view(emb.shape[0],-1) # reshape into a tensor of size (32,30)\n","'''\n","d_emb = d_embcat.view(emb.shape)\n","\n","'''\n","emb = C[x_train_batch] # embed the characters into vectors\n","'''\n","d_C = torch.zeros_like(C)\n","for k in range(x_train_batch.shape[0]):\n","    for j in range(x_train_batch.shape[1]):\n","        ix = x_train_batch[k,j]\n","        d_C[ix] += d_emb[k,j]\n","\n","cmp('h', d_h, h)\n","cmp('W2', d_W2, W2)\n","cmp('b2', d_b2, b2)\n","cmp('hpreact', d_hidden_pre_act, hidden_pre_act)\n","cmp('bngain', d_bngain, bngain)\n","cmp('bnbias', d_bnbias, bnbias)\n","cmp('bnraw', d_bnraw, bnraw)\n","cmp('bnvar_inv', d_bnvar_inv, bnvar_inv)\n","cmp('bnvar', d_bnvar, bnvar)\n","cmp('bndiff2', d_bndiff2, bndiff2)\n","cmp('bndiff', d_bndiff, bndiff)\n","cmp('bnmeani', d_bnmeani, bnmeani)\n","cmp('hprebn', d_hidden_pre_batchnorm, hidden_pre_batchnorm)\n","cmp('embcat', d_embcat, embcat)\n","cmp('W1', d_W1, W1)\n","cmp('b1', d_b1, b1)\n","cmp('emb', d_emb, emb)\n","cmp('C', d_C, C)"],"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["h               | exact: False | approximate: True  | maxdiff: 1.6298145055770874e-09\nW2              | exact: False | approximate: True  | maxdiff: 1.30385160446167e-08\nb2              | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\nhpreact         | exact: False | approximate: True  | maxdiff: 1.57160684466362e-09\nbngain          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\nbnbias          | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\nbnraw           | exact: False | approximate: True  | maxdiff: 1.6298145055770874e-09\nbnvar_inv       | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\nbnvar           | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-09\nbndiff2         | exact: False | approximate: True  | maxdiff: 3.637978807091713e-11\nbndiff          | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-09\nbnmeani         | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\nhprebn          | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-09\nembcat          | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\nW1              | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\nb1              | exact: False | approximate: True  | maxdiff: 6.28642737865448e-09\nemb             | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\nC               | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"]}]},{"cell_type":"code","metadata":{"originalKey":"3c620094-8b3e-4e85-b0cc-137f30e979fa","showInput":true,"customInput":null},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"originalKey":"b1f00a91-19a9-4a55-bda8-8e4bea80d97f","showInput":true,"customInput":null},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"originalKey":"b66080ad-db8a-4f6d-9881-a0df71a0d4bd","showInput":false,"customInput":null},"source":["## train model with manual backprop"]},{"cell_type":"code","metadata":{"originalKey":"4c86e43c-8578-410c-a9f6-346614f66bf7","showInput":true,"customInput":null,"executionStartTime":1707002274994,"executionStopTime":1707002416653,"serverExecutionDuration":141520.38801601,"collapsed":false,"requestMsgId":"19cc8ba6-e91c-460b-b2a8-5045fc762604","outputsInitialized":true,"output":{"id":"368783155784803"}},"source":["# Train the MLP neural net with your own backward pass\n","\n","# init\n","n_embd = 10 # the dimensionality of the character embedding vectors\n","n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n","\n","g = torch.Generator().manual_seed(2147483647) # for reproducibility\n","C  = torch.randn((vocab_size, n_embd),            generator=g)\n","# Layer 1\n","W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5\/3)\/((n_embd * block_size)**0.5)\n","b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n","# Layer 2\n","W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n","b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n","# BatchNorm parameters\n","bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n","bnbias = torch.randn((1, n_hidden))*0.1\n","\n","parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n","print(sum(p.nelement() for p in parameters)) # number of parameters in total\n","for p in parameters:\n","  p.requires_grad = True\n","\n","# same optimization as last time\n","max_steps = 50000\n","batch_size = 32\n","n = batch_size # convenience\n","lossi = []\n","\n","# use this context manager for efficiency once your backward pass is written (TODO)\n","with torch.no_grad():\n","\n","  # kick off optimization\n","  for i in range(max_steps):\n","\n","    # minibatch construct\n","    ix = torch.randint(0, x_train.shape[0], (batch_size,), generator=g)\n","    Xb, Yb = x_train[ix], y_train[ix] # batch X,Y\n","\n","    # forward pass\n","    emb = C[Xb] # embed the characters into vectors\n","    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n","    # Linear layer\n","    hprebn = embcat \u0040 W1 + b1 # hidden layer pre-activation\n","    # BatchNorm layer\n","    # -------------------------------------------------------------\n","    bnmean = hprebn.mean(0, keepdim=True)\n","    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n","    bnvar_inv = (bnvar + 1e-5)**-0.5\n","    bnraw = (hprebn - bnmean) * bnvar_inv\n","    hpreact = bngain * bnraw + bnbias\n","    # -------------------------------------------------------------\n","    # Non-linearity\n","    h = torch.tanh(hpreact) # hidden layer\n","    logits = h \u0040 W2 + b2 # output layer\n","    loss = F.cross_entropy(logits, Yb) # loss function\n","\n","    # backward pass\n","    for p in parameters:\n","      p.grad = None\n","    #loss.backward() # use this for correctness comparisons, delete it later!\n","\n","    # manual backprop! #swole_doge_meme\n","    # -----------------\n","    dlogits = F.softmax(logits, 1)\n","    dlogits[range(n), Yb] -= 1\n","    dlogits \/= n\n","    # 2nd layer backprop\n","    dh = dlogits \u0040 W2.T\n","    dW2 = h.T \u0040 dlogits\n","    db2 = dlogits.sum(0)\n","    # tanh\n","    dhpreact = (1.0 - h**2) * dh\n","    # batchnorm backprop\n","    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n","    dbnbias = dhpreact.sum(0, keepdim=True)\n","    dhprebn = bngain*bnvar_inv\/n * (n*dhpreact - dhpreact.sum(0) - n\/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n","    # 1st layer\n","    dembcat = dhprebn \u0040 W1.T\n","    dW1 = embcat.T \u0040 dhprebn\n","    db1 = dhprebn.sum(0)\n","    # embedding\n","    demb = dembcat.view(emb.shape)\n","    dC = torch.zeros_like(C)\n","    for k in range(Xb.shape[0]):\n","      for j in range(Xb.shape[1]):\n","        ix = Xb[k,j]\n","        dC[ix] += demb[k,j]\n","    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n","    # -----------------\n","\n","    # update\n","    lr = 0.1 if i \u003C 100000 else 0.01 # step learning rate decay\n","    for p, grad in zip(parameters, grads):\n","      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n","      p.data += -lr * grad # new way of swole doge TODO: enable\n","\n","    # track stats\n","    if i \u0025 10000 == 0: # print every once in a while\n","      print(f'{i:7d}\/{max_steps:7d}: {loss.item():.4f}')\n","    lossi.append(loss.log10().item())\n","\n","  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n","  #     break"],"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["12297\n      0\/  50000: 3.7521\n"]},{"output_type":"stream","name":"stdout","text":["  10000\/  50000: 1.8142\n"]},{"output_type":"stream","name":"stdout","text":["  20000\/  50000: 1.8368\n"]},{"output_type":"stream","name":"stdout","text":["  30000\/  50000: 2.4078\n"]},{"output_type":"stream","name":"stdout","text":["  40000\/  50000: 2.4239\n"]}]},{"cell_type":"code","metadata":{"originalKey":"dfdef97f-2a7d-4123-bd4f-dce49c00fc64","showInput":true,"customInput":null,"executionStartTime":1707002484440,"executionStopTime":1707002484675,"serverExecutionDuration":34.40446901368,"collapsed":false,"requestMsgId":"02589261-eeb1-453e-bd2c-ee692c33c1fa","outputsInitialized":true,"output":{"id":"742179177845514"}},"source":["# sample from the model\n","g = torch.Generator().manual_seed(2147483647 + 10)\n","\n","for _ in range(20):\n","    \n","    out = []\n","    context = [0] * block_size # initialize with all ...\n","    while True:\n","      # ------------\n","      # forward pass:\n","      # Embedding\n","      emb = C[torch.tensor([context])] # (1,block_size,d)      \n","      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n","      hpreact = embcat \u0040 W1 + b1\n","      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n","      h = torch.tanh(hpreact) # (N, n_hidden)\n","      logits = h \u0040 W2 + b2 # (N, vocab_size)\n","      # ------------\n","      # Sample\n","      probs = F.softmax(logits, dim=1)\n","      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n","      context = context[1:] + [ix]\n","      out.append(ix)\n","      if ix == 0:\n","        break\n","    \n","    print(''.join(int_to_str[i] for i in out))"],"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["carlaviano.\nhavith.\nmili.\ntaty.\nsalani.\nemmahnelle.\ndiah.\npareei.\nnellairickaiiv.\nkaleigh.\nham.\npoin.\nquinn.\nsulio.\nalianni.\nwatell.\ndearixi.\njace.\npirra.\nmel.\n"]}]},{"cell_type":"code","metadata":{"originalKey":"d075e899-b1c1-4cbb-996e-5a6bc2993bf4","showInput":true,"customInput":null},"source":[""],"execution_count":null,"outputs":[]}]}